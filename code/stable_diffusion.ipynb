{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "#TODO: failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "972fd18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(3),\n",
    "    ToTensor()\n",
    "])\n",
    "raw_training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transformer\n",
    ")\n",
    "\n",
    "raw_test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transformer\n",
    ")\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "174a4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 1e-3\n",
    "    beta_end = 2e-1\n",
    "    return torch.linspace(beta_start,beta_end, timesteps)\n",
    "\n",
    "\n",
    "def forward_diffusion(img, t, noise_schedule):\n",
    "    # Gather beta values for each timestep t (shape: [batch])\n",
    "    beta_t = noise_schedule[t].reshape(-1, 1, 1, 1)  # reshape for broadcasting\n",
    "\n",
    "    noise = torch.randn_like(img)\n",
    "    return torch.sqrt(1 - beta_t) * img + torch.sqrt(beta_t) * noise\n",
    "\n",
    "\n",
    "def forward (img: torch.Tensor,noise_schedule):\n",
    "    for step in range(len(noise_schedule)):\n",
    "        img = forward_diffusion(img,step,noise_schedule)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15861358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(torch.nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, features = [64,128,256,512]):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.ModuleList()\n",
    "        for feature in features:\n",
    "            self.encoder.append(self.conv_block(in_channels,feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        self.bottleneck = self.conv_block(features[-1], features[-1] * 2)\n",
    "\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        for feature in reversed(features):\n",
    "            self.decoder.append(torch.nn.ConvTranspose2d(feature *2 , feature, kernel_size=2, stride=2))\n",
    "            self.decoder.append(self.conv_block(feature*2, feature))\n",
    "        self.final_layer = torch.nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "    def conv_block(self,in_channels, out_channels):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels,kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels,out_channels,kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skip_connections.append(x)\n",
    "            x = torch.nn.MaxPool2d(kernel_size=2, stride=2)(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.decoder), 2):\n",
    "            x = self.decoder[idx](x)  # ConvTranspose2d\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "\n",
    "            # Resize if needed\n",
    "            if x.shape[2:] != skip_connection.shape[2:]:\n",
    "                x = torch.nn.functional.interpolate(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            x = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.decoder[idx + 1](x)  # conv_block\n",
    "\n",
    "        return self.final_layer(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ee6c8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(torch.nn.Module):\n",
    "    def __init__(self,in_channels, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, 64, kernel_size=4,stride= 2, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,latent_dim,kernel_size=4,stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class VAEDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim,out_channels):\n",
    "        super().__init__()\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(latent_dim, 128, kernel_size=4,stride= 2, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(128,64,kernel_size=4,stride=2,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(64,latent_dim,kernel_size=4,stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self,in_channels,latent_dim, out_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = VAEEncoder(in_channels, latent_dim)\n",
    "        self.decoder = VAEDecoder(latent_dim, out_channels)\n",
    "    def forward(self,x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e330136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "model = Unet(in_channels=3, out_channels=3)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataloader = torch.utils.data.DataLoader(raw_training_data, batch_size=64, shuffle=True)\n",
    "\n",
    "epochs = 100\n",
    "timesteps = 10\n",
    "beta_schedule = linear_beta_schedule(timesteps)  # Should return a tensor of shape [timesteps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "83c787c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9952\n",
      "Epoch 2, Loss: 0.9724\n",
      "Epoch 3, Loss: 1.0302\n",
      "Epoch 4, Loss: 0.9346\n",
      "Epoch 5, Loss: 0.9871\n",
      "Epoch 6, Loss: 1.0113\n",
      "Epoch 7, Loss: 1.0047\n",
      "Epoch 8, Loss: 1.0241\n",
      "Epoch 9, Loss: 0.9559\n",
      "Epoch 10, Loss: 1.0315\n",
      "Epoch 11, Loss: 0.9709\n",
      "Epoch 12, Loss: 1.0269\n",
      "Epoch 13, Loss: 1.0217\n",
      "Epoch 14, Loss: 1.0118\n",
      "Epoch 15, Loss: 0.9822\n",
      "Epoch 16, Loss: 0.9764\n",
      "Epoch 17, Loss: 0.9408\n",
      "Epoch 18, Loss: 0.9800\n",
      "Epoch 19, Loss: 1.0262\n",
      "Epoch 20, Loss: 1.0028\n",
      "Epoch 21, Loss: 1.0289\n",
      "Epoch 22, Loss: 0.9501\n",
      "Epoch 23, Loss: 1.0096\n",
      "Epoch 24, Loss: 0.9682\n",
      "Epoch 25, Loss: 1.0119\n",
      "Epoch 26, Loss: 1.0207\n",
      "Epoch 27, Loss: 0.9735\n",
      "Epoch 28, Loss: 1.0412\n",
      "Epoch 29, Loss: 0.9648\n",
      "Epoch 30, Loss: 0.9831\n",
      "Epoch 31, Loss: 1.0238\n",
      "Epoch 32, Loss: 0.9903\n",
      "Epoch 33, Loss: 0.9964\n",
      "Epoch 34, Loss: 1.0195\n",
      "Epoch 35, Loss: 0.9877\n",
      "Epoch 36, Loss: 1.0057\n",
      "Epoch 37, Loss: 0.9701\n",
      "Epoch 38, Loss: 1.0341\n",
      "Epoch 39, Loss: 1.0307\n",
      "Epoch 40, Loss: 1.0360\n",
      "Epoch 41, Loss: 1.0012\n",
      "Epoch 42, Loss: 0.9886\n",
      "Epoch 43, Loss: 0.9928\n",
      "Epoch 44, Loss: 1.0086\n",
      "Epoch 45, Loss: 1.0131\n",
      "Epoch 46, Loss: 1.0321\n",
      "Epoch 47, Loss: 1.0205\n",
      "Epoch 48, Loss: 0.9625\n",
      "Epoch 49, Loss: 1.0273\n",
      "Epoch 50, Loss: 0.9912\n",
      "Epoch 51, Loss: 1.0159\n",
      "Epoch 52, Loss: 1.0333\n",
      "Epoch 53, Loss: 1.0269\n",
      "Epoch 54, Loss: 1.0038\n",
      "Epoch 55, Loss: 1.0031\n",
      "Epoch 56, Loss: 1.0039\n",
      "Epoch 57, Loss: 1.0046\n",
      "Epoch 58, Loss: 1.0011\n",
      "Epoch 59, Loss: 0.9568\n",
      "Epoch 60, Loss: 0.9844\n",
      "Epoch 61, Loss: 0.9833\n",
      "Epoch 62, Loss: 0.9604\n",
      "Epoch 63, Loss: 1.0127\n",
      "Epoch 64, Loss: 0.9607\n",
      "Epoch 65, Loss: 1.0321\n",
      "Epoch 66, Loss: 0.9401\n",
      "Epoch 67, Loss: 0.9719\n",
      "Epoch 68, Loss: 1.0078\n",
      "Epoch 69, Loss: 0.9938\n",
      "Epoch 70, Loss: 0.9936\n",
      "Epoch 71, Loss: 0.9970\n",
      "Epoch 72, Loss: 1.0109\n",
      "Epoch 73, Loss: 1.0259\n",
      "Epoch 74, Loss: 0.9839\n",
      "Epoch 75, Loss: 0.9915\n",
      "Epoch 76, Loss: 1.0026\n",
      "Epoch 77, Loss: 1.0258\n",
      "Epoch 78, Loss: 0.9758\n",
      "Epoch 79, Loss: 1.0067\n",
      "Epoch 80, Loss: 1.0024\n",
      "Epoch 81, Loss: 1.0341\n",
      "Epoch 82, Loss: 1.0379\n",
      "Epoch 83, Loss: 0.9307\n",
      "Epoch 84, Loss: 1.0152\n",
      "Epoch 85, Loss: 0.9959\n",
      "Epoch 86, Loss: 1.0134\n",
      "Epoch 87, Loss: 1.0074\n",
      "Epoch 88, Loss: 1.0036\n",
      "Epoch 89, Loss: 0.9697\n",
      "Epoch 90, Loss: 1.0124\n",
      "Epoch 91, Loss: 1.0018\n",
      "Epoch 92, Loss: 1.0115\n",
      "Epoch 93, Loss: 0.9826\n",
      "Epoch 94, Loss: 0.9834\n",
      "Epoch 95, Loss: 0.9673\n",
      "Epoch 96, Loss: 0.9886\n",
      "Epoch 97, Loss: 1.0164\n",
      "Epoch 98, Loss: 0.9817\n",
      "Epoch 99, Loss: 1.0260\n",
      "Epoch 100, Loss: 1.0011\n",
      "Epoch 101, Loss: 1.0163\n",
      "Epoch 102, Loss: 1.0105\n",
      "Epoch 103, Loss: 1.0150\n",
      "Epoch 104, Loss: 1.0168\n",
      "Epoch 105, Loss: 0.9757\n",
      "Epoch 106, Loss: 0.9781\n",
      "Epoch 107, Loss: 1.0116\n",
      "Epoch 108, Loss: 1.0551\n",
      "Epoch 109, Loss: 1.0066\n",
      "Epoch 110, Loss: 0.9854\n",
      "Epoch 111, Loss: 0.9911\n",
      "Epoch 112, Loss: 0.9801\n",
      "Epoch 113, Loss: 1.0149\n",
      "Epoch 114, Loss: 0.9546\n",
      "Epoch 115, Loss: 1.0014\n",
      "Epoch 116, Loss: 1.0447\n",
      "Epoch 117, Loss: 1.0037\n",
      "Epoch 118, Loss: 1.0363\n",
      "Epoch 119, Loss: 0.9927\n",
      "Epoch 120, Loss: 0.9844\n",
      "Epoch 121, Loss: 1.0279\n",
      "Epoch 122, Loss: 0.9779\n",
      "Epoch 123, Loss: 1.0533\n",
      "Epoch 124, Loss: 1.0321\n",
      "Epoch 125, Loss: 1.0386\n",
      "Epoch 126, Loss: 1.0030\n",
      "Epoch 127, Loss: 1.0177\n",
      "Epoch 128, Loss: 1.0527\n",
      "Epoch 129, Loss: 1.0381\n",
      "Epoch 130, Loss: 1.0442\n",
      "Epoch 131, Loss: 0.9513\n",
      "Epoch 132, Loss: 1.0169\n",
      "Epoch 133, Loss: 1.0123\n",
      "Epoch 134, Loss: 0.9992\n",
      "Epoch 135, Loss: 1.0112\n",
      "Epoch 136, Loss: 1.0068\n",
      "Epoch 137, Loss: 0.9572\n",
      "Epoch 138, Loss: 1.0176\n",
      "Epoch 139, Loss: 0.9731\n",
      "Epoch 140, Loss: 1.0058\n",
      "Epoch 141, Loss: 0.9947\n",
      "Epoch 142, Loss: 0.9874\n",
      "Epoch 143, Loss: 1.0472\n",
      "Epoch 144, Loss: 0.9764\n",
      "Epoch 145, Loss: 1.0224\n",
      "Epoch 146, Loss: 0.9835\n",
      "Epoch 147, Loss: 0.9785\n",
      "Epoch 148, Loss: 0.9526\n",
      "Epoch 149, Loss: 1.0133\n",
      "Epoch 150, Loss: 1.0076\n",
      "Epoch 151, Loss: 0.9928\n",
      "Epoch 152, Loss: 1.0065\n",
      "Epoch 153, Loss: 0.9885\n",
      "Epoch 154, Loss: 0.9597\n",
      "Epoch 155, Loss: 0.9900\n",
      "Epoch 156, Loss: 0.9885\n",
      "Epoch 157, Loss: 1.0032\n",
      "Epoch 158, Loss: 1.0228\n",
      "Epoch 159, Loss: 0.9979\n",
      "Epoch 160, Loss: 0.9944\n",
      "Epoch 161, Loss: 1.0077\n",
      "Epoch 162, Loss: 0.9615\n",
      "Epoch 163, Loss: 1.0030\n",
      "Epoch 164, Loss: 1.0553\n",
      "Epoch 165, Loss: 0.9726\n",
      "Epoch 166, Loss: 0.9804\n",
      "Epoch 167, Loss: 0.9956\n",
      "Epoch 168, Loss: 1.0371\n",
      "Epoch 169, Loss: 0.9894\n",
      "Epoch 170, Loss: 0.9912\n",
      "Epoch 171, Loss: 1.0260\n",
      "Epoch 172, Loss: 1.0291\n",
      "Epoch 173, Loss: 1.0181\n",
      "Epoch 174, Loss: 1.0033\n",
      "Epoch 175, Loss: 1.0358\n",
      "Epoch 176, Loss: 1.0116\n",
      "Epoch 177, Loss: 1.0629\n",
      "Epoch 178, Loss: 0.9598\n",
      "Epoch 179, Loss: 1.0506\n",
      "Epoch 180, Loss: 1.0388\n",
      "Epoch 181, Loss: 1.0050\n",
      "Epoch 182, Loss: 0.9880\n",
      "Epoch 183, Loss: 0.9855\n",
      "Epoch 184, Loss: 1.0187\n",
      "Epoch 185, Loss: 0.9747\n",
      "Epoch 186, Loss: 1.0293\n",
      "Epoch 187, Loss: 0.9385\n",
      "Epoch 188, Loss: 1.0121\n",
      "Epoch 189, Loss: 0.9674\n",
      "Epoch 190, Loss: 1.0247\n",
      "Epoch 191, Loss: 0.9742\n",
      "Epoch 192, Loss: 0.9816\n",
      "Epoch 193, Loss: 0.9736\n",
      "Epoch 194, Loss: 1.0345\n",
      "Epoch 195, Loss: 0.9435\n",
      "Epoch 196, Loss: 1.0258\n",
      "Epoch 197, Loss: 0.9542\n",
      "Epoch 198, Loss: 0.9602\n",
      "Epoch 199, Loss: 1.0210\n",
      "Epoch 200, Loss: 0.9976\n",
      "Epoch 201, Loss: 1.0049\n",
      "Epoch 202, Loss: 1.0182\n",
      "Epoch 203, Loss: 0.9550\n",
      "Epoch 204, Loss: 1.0016\n",
      "Epoch 205, Loss: 1.0368\n",
      "Epoch 206, Loss: 1.0272\n",
      "Epoch 207, Loss: 0.9619\n",
      "Epoch 208, Loss: 0.9504\n",
      "Epoch 209, Loss: 0.9590\n",
      "Epoch 210, Loss: 0.9527\n",
      "Epoch 211, Loss: 1.0070\n",
      "Epoch 212, Loss: 0.9540\n",
      "Epoch 213, Loss: 1.0088\n",
      "Epoch 214, Loss: 0.9887\n",
      "Epoch 215, Loss: 1.0145\n",
      "Epoch 216, Loss: 0.9966\n",
      "Epoch 217, Loss: 0.9573\n",
      "Epoch 218, Loss: 0.9931\n",
      "Epoch 219, Loss: 0.9822\n",
      "Epoch 220, Loss: 0.9911\n",
      "Epoch 221, Loss: 0.9634\n",
      "Epoch 222, Loss: 1.0190\n",
      "Epoch 223, Loss: 1.0196\n",
      "Epoch 224, Loss: 0.9663\n",
      "Epoch 225, Loss: 0.9727\n",
      "Epoch 226, Loss: 1.0152\n",
      "Epoch 227, Loss: 0.9749\n",
      "Epoch 228, Loss: 1.0211\n",
      "Epoch 229, Loss: 0.9259\n",
      "Epoch 230, Loss: 1.0118\n",
      "Epoch 231, Loss: 0.9963\n",
      "Epoch 232, Loss: 1.0077\n",
      "Epoch 233, Loss: 1.0297\n",
      "Epoch 234, Loss: 1.0146\n",
      "Epoch 235, Loss: 0.9884\n",
      "Epoch 236, Loss: 1.0314\n",
      "Epoch 237, Loss: 0.9822\n",
      "Epoch 238, Loss: 0.9768\n",
      "Epoch 239, Loss: 0.9915\n",
      "Epoch 240, Loss: 0.9489\n",
      "Epoch 241, Loss: 1.0197\n",
      "Epoch 242, Loss: 0.9965\n",
      "Epoch 243, Loss: 1.0221\n",
      "Epoch 244, Loss: 0.9857\n",
      "Epoch 245, Loss: 0.9734\n",
      "Epoch 246, Loss: 1.0035\n",
      "Epoch 247, Loss: 1.0119\n",
      "Epoch 248, Loss: 0.9765\n",
      "Epoch 249, Loss: 1.0136\n",
      "Epoch 250, Loss: 1.0056\n",
      "Epoch 251, Loss: 0.9937\n",
      "Epoch 252, Loss: 0.9941\n",
      "Epoch 253, Loss: 0.9937\n",
      "Epoch 254, Loss: 1.0182\n",
      "Epoch 255, Loss: 0.9907\n",
      "Epoch 256, Loss: 0.9919\n",
      "Epoch 257, Loss: 1.0503\n",
      "Epoch 258, Loss: 1.0436\n",
      "Epoch 259, Loss: 0.9735\n",
      "Epoch 260, Loss: 0.9452\n",
      "Epoch 261, Loss: 1.0231\n",
      "Epoch 262, Loss: 1.0182\n",
      "Epoch 263, Loss: 1.0030\n",
      "Epoch 264, Loss: 1.0206\n",
      "Epoch 265, Loss: 1.0049\n",
      "Epoch 266, Loss: 1.0451\n",
      "Epoch 267, Loss: 0.9980\n",
      "Epoch 268, Loss: 0.9809\n",
      "Epoch 269, Loss: 0.9938\n",
      "Epoch 270, Loss: 1.0284\n",
      "Epoch 271, Loss: 0.9831\n",
      "Epoch 272, Loss: 0.9974\n",
      "Epoch 273, Loss: 0.9774\n",
      "Epoch 274, Loss: 0.9677\n",
      "Epoch 275, Loss: 0.9782\n",
      "Epoch 276, Loss: 1.0114\n",
      "Epoch 277, Loss: 0.9968\n",
      "Epoch 278, Loss: 1.0205\n",
      "Epoch 279, Loss: 1.0056\n",
      "Epoch 280, Loss: 0.9660\n",
      "Epoch 281, Loss: 0.9872\n",
      "Epoch 282, Loss: 0.9721\n",
      "Epoch 283, Loss: 0.9674\n",
      "Epoch 284, Loss: 0.9445\n",
      "Epoch 285, Loss: 0.9713\n",
      "Epoch 286, Loss: 1.0163\n",
      "Epoch 287, Loss: 1.0418\n",
      "Epoch 288, Loss: 0.9901\n",
      "Epoch 289, Loss: 1.0053\n",
      "Epoch 290, Loss: 1.0705\n",
      "Epoch 291, Loss: 0.9646\n",
      "Epoch 292, Loss: 1.0029\n",
      "Epoch 293, Loss: 1.0060\n",
      "Epoch 294, Loss: 1.0423\n",
      "Epoch 295, Loss: 1.0227\n",
      "Epoch 296, Loss: 0.9878\n",
      "Epoch 297, Loss: 0.9691\n",
      "Epoch 298, Loss: 0.9543\n",
      "Epoch 299, Loss: 0.9958\n",
      "Epoch 300, Loss: 0.9703\n",
      "Epoch 301, Loss: 0.9620\n",
      "Epoch 302, Loss: 1.0012\n",
      "Epoch 303, Loss: 0.9805\n",
      "Epoch 304, Loss: 0.9553\n",
      "Epoch 305, Loss: 1.0067\n",
      "Epoch 306, Loss: 0.9858\n",
      "Epoch 307, Loss: 0.9858\n",
      "Epoch 308, Loss: 1.0392\n",
      "Epoch 309, Loss: 0.9657\n",
      "Epoch 310, Loss: 0.9556\n",
      "Epoch 311, Loss: 0.9836\n",
      "Epoch 312, Loss: 0.9935\n",
      "Epoch 313, Loss: 0.9913\n",
      "Epoch 314, Loss: 1.0291\n",
      "Epoch 315, Loss: 1.0088\n",
      "Epoch 316, Loss: 0.9729\n",
      "Epoch 317, Loss: 1.0346\n",
      "Epoch 318, Loss: 1.0276\n",
      "Epoch 319, Loss: 1.0329\n",
      "Epoch 320, Loss: 1.0099\n",
      "Epoch 321, Loss: 1.0008\n",
      "Epoch 322, Loss: 1.0179\n",
      "Epoch 323, Loss: 0.9612\n",
      "Epoch 324, Loss: 1.0274\n",
      "Epoch 325, Loss: 1.0043\n",
      "Epoch 326, Loss: 1.0313\n",
      "Epoch 327, Loss: 1.0071\n",
      "Epoch 328, Loss: 0.9995\n",
      "Epoch 329, Loss: 0.9979\n",
      "Epoch 330, Loss: 1.0266\n",
      "Epoch 331, Loss: 1.0242\n",
      "Epoch 332, Loss: 0.9812\n",
      "Epoch 333, Loss: 0.9664\n",
      "Epoch 334, Loss: 1.0173\n",
      "Epoch 335, Loss: 1.0217\n",
      "Epoch 336, Loss: 0.9889\n",
      "Epoch 337, Loss: 0.9524\n",
      "Epoch 338, Loss: 1.0264\n",
      "Epoch 339, Loss: 0.9732\n",
      "Epoch 340, Loss: 0.9749\n",
      "Epoch 341, Loss: 0.9780\n",
      "Epoch 342, Loss: 1.0422\n",
      "Epoch 343, Loss: 1.0432\n",
      "Epoch 344, Loss: 1.0175\n",
      "Epoch 345, Loss: 1.0111\n",
      "Epoch 346, Loss: 1.0499\n",
      "Epoch 347, Loss: 1.0008\n",
      "Epoch 348, Loss: 1.0436\n",
      "Epoch 349, Loss: 0.9465\n",
      "Epoch 350, Loss: 1.0927\n",
      "Epoch 351, Loss: 0.9677\n",
      "Epoch 352, Loss: 1.0118\n",
      "Epoch 353, Loss: 0.9921\n",
      "Epoch 354, Loss: 0.9985\n",
      "Epoch 355, Loss: 1.0084\n",
      "Epoch 356, Loss: 0.9712\n",
      "Epoch 357, Loss: 1.0406\n",
      "Epoch 358, Loss: 0.9862\n",
      "Epoch 359, Loss: 1.0054\n",
      "Epoch 360, Loss: 0.9879\n",
      "Epoch 361, Loss: 0.9776\n",
      "Epoch 362, Loss: 0.9857\n",
      "Epoch 363, Loss: 0.9939\n",
      "Epoch 364, Loss: 1.0214\n",
      "Epoch 365, Loss: 0.9537\n",
      "Epoch 366, Loss: 1.0084\n",
      "Epoch 367, Loss: 0.9952\n",
      "Epoch 368, Loss: 0.9643\n",
      "Epoch 369, Loss: 0.9985\n",
      "Epoch 370, Loss: 1.0181\n",
      "Epoch 371, Loss: 0.9637\n",
      "Epoch 372, Loss: 0.9527\n",
      "Epoch 373, Loss: 1.0389\n",
      "Epoch 374, Loss: 1.0024\n",
      "Epoch 375, Loss: 0.9921\n",
      "Epoch 376, Loss: 0.9875\n",
      "Epoch 377, Loss: 0.9683\n",
      "Epoch 378, Loss: 0.9980\n",
      "Epoch 379, Loss: 1.0595\n",
      "Epoch 380, Loss: 1.0008\n",
      "Epoch 381, Loss: 0.9912\n",
      "Epoch 382, Loss: 0.9497\n",
      "Epoch 383, Loss: 0.9882\n",
      "Epoch 384, Loss: 0.9605\n",
      "Epoch 385, Loss: 0.9522\n",
      "Epoch 386, Loss: 1.0061\n",
      "Epoch 387, Loss: 0.9672\n",
      "Epoch 388, Loss: 0.9253\n",
      "Epoch 389, Loss: 0.9808\n",
      "Epoch 390, Loss: 1.0269\n",
      "Epoch 391, Loss: 1.0085\n",
      "Epoch 392, Loss: 0.9345\n",
      "Epoch 393, Loss: 0.9981\n",
      "Epoch 394, Loss: 1.0109\n",
      "Epoch 395, Loss: 1.0104\n",
      "Epoch 396, Loss: 1.0311\n",
      "Epoch 397, Loss: 0.9486\n",
      "Epoch 398, Loss: 0.9864\n",
      "Epoch 399, Loss: 0.9887\n",
      "Epoch 400, Loss: 1.0199\n",
      "Epoch 401, Loss: 0.9428\n",
      "Epoch 402, Loss: 1.0116\n",
      "Epoch 403, Loss: 1.0464\n",
      "Epoch 404, Loss: 0.9810\n",
      "Epoch 405, Loss: 0.9832\n",
      "Epoch 406, Loss: 1.0479\n",
      "Epoch 407, Loss: 1.0392\n",
      "Epoch 408, Loss: 0.9623\n",
      "Epoch 409, Loss: 1.0029\n",
      "Epoch 410, Loss: 1.0403\n",
      "Epoch 411, Loss: 1.0204\n",
      "Epoch 412, Loss: 0.9995\n",
      "Epoch 413, Loss: 0.9921\n",
      "Epoch 414, Loss: 1.0138\n",
      "Epoch 415, Loss: 1.0038\n",
      "Epoch 416, Loss: 0.9950\n",
      "Epoch 417, Loss: 1.0306\n",
      "Epoch 418, Loss: 1.0079\n",
      "Epoch 419, Loss: 1.0514\n",
      "Epoch 420, Loss: 0.9882\n",
      "Epoch 421, Loss: 0.9799\n",
      "Epoch 422, Loss: 0.9938\n",
      "Epoch 423, Loss: 0.9786\n",
      "Epoch 424, Loss: 1.0107\n",
      "Epoch 425, Loss: 1.0071\n",
      "Epoch 426, Loss: 0.9884\n",
      "Epoch 427, Loss: 1.0000\n",
      "Epoch 428, Loss: 0.9973\n",
      "Epoch 429, Loss: 0.9943\n",
      "Epoch 430, Loss: 0.9777\n",
      "Epoch 431, Loss: 0.9962\n",
      "Epoch 432, Loss: 1.0036\n",
      "Epoch 433, Loss: 1.0101\n",
      "Epoch 434, Loss: 1.0039\n",
      "Epoch 435, Loss: 0.9974\n",
      "Epoch 436, Loss: 1.0254\n",
      "Epoch 437, Loss: 1.0024\n",
      "Epoch 438, Loss: 1.0269\n",
      "Epoch 439, Loss: 1.0696\n",
      "Epoch 440, Loss: 1.0062\n",
      "Epoch 441, Loss: 1.0022\n",
      "Epoch 442, Loss: 0.9781\n",
      "Epoch 443, Loss: 0.9861\n",
      "Epoch 444, Loss: 0.9950\n",
      "Epoch 445, Loss: 1.0412\n",
      "Epoch 446, Loss: 0.9947\n",
      "Epoch 447, Loss: 1.0264\n",
      "Epoch 448, Loss: 1.0060\n",
      "Epoch 449, Loss: 0.9911\n",
      "Epoch 450, Loss: 1.0409\n",
      "Epoch 451, Loss: 1.0241\n",
      "Epoch 452, Loss: 1.0253\n",
      "Epoch 453, Loss: 0.9646\n",
      "Epoch 454, Loss: 0.9941\n",
      "Epoch 455, Loss: 0.9912\n",
      "Epoch 456, Loss: 1.0214\n",
      "Epoch 457, Loss: 1.0449\n",
      "Epoch 458, Loss: 1.0482\n",
      "Epoch 459, Loss: 0.9933\n",
      "Epoch 460, Loss: 0.9635\n",
      "Epoch 461, Loss: 0.9852\n",
      "Epoch 462, Loss: 1.0352\n",
      "Epoch 463, Loss: 0.9688\n",
      "Epoch 464, Loss: 0.9795\n",
      "Epoch 465, Loss: 0.9990\n",
      "Epoch 466, Loss: 0.9764\n",
      "Epoch 467, Loss: 1.0915\n",
      "Epoch 468, Loss: 1.0516\n",
      "Epoch 469, Loss: 1.0280\n",
      "Epoch 470, Loss: 1.0342\n",
      "Epoch 471, Loss: 1.0202\n",
      "Epoch 472, Loss: 1.0154\n",
      "Epoch 473, Loss: 0.9745\n",
      "Epoch 474, Loss: 1.0164\n",
      "Epoch 475, Loss: 1.0117\n",
      "Epoch 476, Loss: 0.9717\n",
      "Epoch 477, Loss: 0.9902\n",
      "Epoch 478, Loss: 0.9489\n",
      "Epoch 479, Loss: 1.0040\n",
      "Epoch 480, Loss: 0.9631\n",
      "Epoch 481, Loss: 1.0151\n",
      "Epoch 482, Loss: 1.0122\n",
      "Epoch 483, Loss: 1.0031\n",
      "Epoch 484, Loss: 0.9365\n",
      "Epoch 485, Loss: 0.9784\n",
      "Epoch 486, Loss: 0.9958\n",
      "Epoch 487, Loss: 0.9493\n",
      "Epoch 488, Loss: 1.0095\n",
      "Epoch 489, Loss: 0.9851\n",
      "Epoch 490, Loss: 0.9956\n",
      "Epoch 491, Loss: 0.9965\n",
      "Epoch 492, Loss: 1.0682\n",
      "Epoch 493, Loss: 0.9815\n",
      "Epoch 494, Loss: 1.0193\n",
      "Epoch 495, Loss: 1.0268\n",
      "Epoch 496, Loss: 1.0199\n",
      "Epoch 497, Loss: 0.9966\n",
      "Epoch 498, Loss: 0.9849\n",
      "Epoch 499, Loss: 0.9630\n",
      "Epoch 500, Loss: 0.9580\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    \n",
    "    step =0\n",
    "\n",
    "    for img, _ in [raw_training_data[0]]:\n",
    "        \n",
    "        img = img.to(torch.float32)  # Ensure float for model and noise\n",
    "        t = torch.randint(0, timesteps, (img.shape[0],), device=img.device)  # (batch,) shape\n",
    "\n",
    "        noise = torch.randn_like(img)\n",
    "        noisy_img = forward_diffusion(img, t, beta_schedule)  # Assumes function adds noise based on t\n",
    "\n",
    "        predicted_noise = model(noisy_img)\n",
    "\n",
    "        loss = loss_fn(predicted_noise, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a6a64716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise_image = torch.rand((1, 3, 28, 28))\n",
    "denoised_img = noise_image.clone()  # Start from noisy image\n",
    "\n",
    "for i in range(10):\n",
    "    denoised_img = denoised_img - model(denoised_img)  # Subtract predicted noise\n",
    "\n",
    "# Convert for display: [1, 3, 28, 28] → [28, 28, 3]\n",
    "img = denoised_img.squeeze(0).permute(1, 2, 0).detach().clamp(0, 1)  # Clamp to [0,1] for visualization\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
